
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{note2}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \subsection{Introduction to Ensembling/Stacking in
Python}\label{introduction-to-ensemblingstacking-in-python}

\subsubsection{Introduction}\label{introduction}

This notebook is a very basic and simple introductory primer to the
method of ensembling (combining) base learning models, in particular the
variant of ensembling known as Stacking. In a nutshell stacking uses as
a first-level (base), the predictions of a few basic classifiers and
then uses another model at the second-level to predict the output from
the earlier first-level predictions.

The Titanic dataset is a prime candidate for introducing this concept as
many newcomers to Kaggle start out here. Furthermore even though
stacking has been responsible for many a team winning Kaggle
competitions there seems to be a dearth of kernels on this topic so I
hope this notebook can fill somewhat of that void.

I myself am quite a newcomer to the Kaggle scene as well and the first
proper ensembling/stacking script that I managed to chance upon and
study was one written in the AllState Severity Claims competition by the
great Faron. The material in this notebook borrows heavily from Faron's
script although ported to factor in ensembles of classifiers whilst his
was ensembles of regressors. Anyway please check out his script here:

Stacking Starter : by Faron

Now onto the notebook at hand and I hope that it manages to do justice
and convey the concept of ensembling in an intuitive and concise manner.
My other standalone Kaggle script which implements exactly the same
ensembling steps (albeit with different parameters) discussed below
gives a Public LB score of 0.808 which is good enough to get to the top
9\% and runs just under 4 minutes. Therefore I am pretty sure there is a
lot of room to improve and add on to that script. Anyways please feel
free to leave me any comments with regards to how I can improve

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Load in our libraries}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{re}
        \PY{k+kn}{import} \PY{n+nn}{sklearn}
        \PY{k+kn}{import} \PY{n+nn}{xgboost} \PY{k}{as} \PY{n+nn}{xgb}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{offline} \PY{k}{as} \PY{n+nn}{py}
        \PY{n}{py}\PY{o}{.}\PY{n}{init\PYZus{}notebook\PYZus{}mode}\PY{p}{(}\PY{n}{connected}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{graph\PYZus{}objs} \PY{k}{as} \PY{n+nn}{go}
        \PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{tools} \PY{k}{as} \PY{n+nn}{tls}
        
        \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Going to use these 5 base models for the stacking}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{p}{(}\PY{n}{RandomForestClassifier}\PY{p}{,} \PY{n}{AdaBoostClassifier}\PY{p}{,} 
                                      \PY{n}{GradientBoostingClassifier}\PY{p}{,} \PY{n}{ExtraTreesClassifier}\PY{p}{)}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVC}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cross\PYZus{}validation} \PY{k}{import} \PY{n}{KFold}
\end{Verbatim}


    
    
    \subsubsection{Feature Exploration, Engineering and
Cleaning}\label{feature-exploration-engineering-and-cleaning}

Now we will proceed much like how most kernels in general are
structured, and that is to first explore the data on hand, identify
possible feature engineering opportunities as well as numerically encode
any categorical features.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Load in the train and test datasets}
        \PY{n}{train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/Users/wubp88/githubLearn/python/data/note2/train.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/Users/wubp88/githubLearn/python/data/note2/test.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Store our passenger ID for easy access}
        \PY{n}{PassengerId} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PassengerId}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{train}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:}    PassengerId  Survived  Pclass  \textbackslash{}
        0            1         0       3   
        1            2         1       1   
        2            3         1       3   
        
                                                        Name     Sex   Age  SibSp  \textbackslash{}
        0                            Braund, Mr. Owen Harris    male  22.0      1   
        1  Cumings, Mrs. John Bradley (Florence Briggs Th{\ldots}  female  38.0      1   
        2                             Heikkinen, Miss. Laina  female  26.0      0   
        
           Parch            Ticket     Fare Cabin Embarked  
        0      0         A/5 21171   7.2500   NaN        S  
        1      0          PC 17599  71.2833   C85        C  
        2      0  STON/O2. 3101282   7.9250   NaN        S  
\end{Verbatim}
            
    Well it is no surprise that our task is to somehow extract the
information out of the categorical variables

\paragraph{Feature Engineering}\label{feature-engineering}

Here, credit must be extended to Sina's very comprehensive and
well-thought out notebook for the feature engineering ideas so please
check out his work

Titanic Best Working Classfier : by Sina

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{full\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{n}{train}\PY{p}{,} \PY{n}{test}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Some features of my own that I have added in}
        \PY{c+c1}{\PYZsh{} Gives the length of the name}
        \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Name\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n+nb}{len}\PY{p}{)}
        \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Name\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n+nb}{len}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Feature that tells whether a passenger had a cabin on the Titanic}
        \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Has\PYZus{}Cabin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cabin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+m+mi}{0} \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{==} \PY{n+nb}{float} \PY{k}{else} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Has\PYZus{}Cabin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cabin}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{l+m+mi}{0} \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{==} \PY{n+nb}{float} \PY{k}{else} \PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Feature engineering steps taken from Sina}
        \PY{c+c1}{\PYZsh{} Create new feature FamilySize as a combination of SibSp and Parch}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FamilySize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SibSp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{+} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Parch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}
        \PY{c+c1}{\PYZsh{} Create new feature IsAlone from FamilySize}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IsAlone}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FamilySize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IsAlone}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{c+c1}{\PYZsh{} Remove all NULLS in the Embarked column}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Embarked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Embarked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Remove all NULLS in the Fare column and create a new feature CategoricalFare}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CategoricalFare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{qcut}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Create a New feature CategoricalAge}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{age\PYZus{}avg} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
            \PY{n}{age\PYZus{}std} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
            \PY{n}{age\PYZus{}null\PYZus{}count} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
            \PY{n}{age\PYZus{}null\PYZus{}random\PYZus{}list} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{age\PYZus{}avg} \PY{o}{\PYZhy{}} \PY{n}{age\PYZus{}std}\PY{p}{,} \PY{n}{age\PYZus{}avg} \PY{o}{+} \PY{n}{age\PYZus{}std}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{age\PYZus{}null\PYZus{}count}\PY{p}{)}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{isnan}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{age\PYZus{}null\PYZus{}random\PYZus{}list}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
        \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CategoricalAge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{cut}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Define function to extract titles from passenger names}
        \PY{k}{def} \PY{n+nf}{get\PYZus{}title}\PY{p}{(}\PY{n}{name}\PY{p}{)}\PY{p}{:}
            \PY{n}{title\PYZus{}search} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ ([A\PYZhy{}Za\PYZhy{}z]+)}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} If the title exists, extract and return it.}
            \PY{k}{if} \PY{n}{title\PYZus{}search}\PY{p}{:}
                \PY{k}{return} \PY{n}{title\PYZus{}search}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{k}{return} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}
        \PY{c+c1}{\PYZsh{} Create a new feature Title, containing the titles of passenger names}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{get\PYZus{}title}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Group all non\PYZhy{}common titles into one single grouping \PYZdq{}Rare\PYZdq{}}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lady}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Countess}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Capt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Col}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Don}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Major}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Rev}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sir}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jonkheer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dona}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Rare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mlle}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Miss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ms}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Miss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mme}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mrs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{for} \PY{n}{dataset} \PY{o+ow}{in} \PY{n}{full\PYZus{}data}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Mapping Sex}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{female}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{male}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}} \PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Mapping titles}
            \PY{n}{title\PYZus{}mapping} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mr}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Miss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mrs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Master}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rare}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{5}\PY{p}{\PYZcb{}}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{title\PYZus{}mapping}\PY{p}{)}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Mapping Embarked}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Embarked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Embarked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Q}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{\PYZcb{}} \PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Mapping Fare}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{7.91}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} 						        \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mf}{7.91}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{14.454}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mf}{14.454}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{31}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}   \PY{o}{=} \PY{l+m+mi}{2}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{31}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} 							        \PY{o}{=} \PY{l+m+mi}{3}
            \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Mapping Age}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{16}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} 					       \PY{o}{=} \PY{l+m+mi}{0}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{16}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{32}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{32}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{48}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{2}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{48}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{64}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{3}
            \PY{n}{dataset}\PY{o}{.}\PY{n}{loc}\PY{p}{[} \PY{n}{dataset}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{64}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{4} \PY{p}{;}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Feature selection}
        \PY{n}{drop\PYZus{}elements} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PassengerId}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ticket}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cabin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SibSp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{drop\PYZus{}elements}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CategoricalAge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CategoricalFare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{test}  \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{drop\PYZus{}elements}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    All right so now having cleaned the features and extracted relevant
information and dropped the categorical columns our features should now
all be numeric, a format suitable to feed into our Machine Learning
models. However before we proceed let us generate some simple
correlation and distribution plots of our transformed dataset to observe
ho

\subsubsection{Visualisations¶}\label{visualisations}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{train}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:}    Survived  Pclass  Sex  Age  Parch  Fare  Embarked  Name\_length  Has\_Cabin  \textbackslash{}
        0         0       3    1    1      0     0         0           23          0   
        1         1       1    0    2      0     3         1           51          1   
        2         1       3    0    1      0     1         0           22          0   
        
           FamilySize  IsAlone  Title  
        0           2        0      1  
        1           2        0      3  
        2           1        1      2  
\end{Verbatim}
            
    \paragraph{Pearson Correlation
Heatmap}\label{pearson-correlation-heatmap}

let us generate some correlation plots of the features to see how
related one feature is to the next. To do so, we will utilise the
Seaborn plotting package which allows us to plot heatmaps very
conveniently as follows

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{colormap} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{RdBu}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Pearson Correlation of Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+m+mf}{1.05}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
        \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{train}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{linewidths}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{vmax}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} 
                    \PY{n}{square}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{colormap}\PY{p}{,} \PY{n}{linecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1a0c30de80>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Takeaway from the Plots}\label{takeaway-from-the-plots}

One thing that that the Pearson Correlation plot can tell us is that
there are not too many features strongly correlated with one another.
This is good from a point of view of feeding these features into your
learning model because this means that there isn't much redundant or
superfluous data in our training set and we are happy that each feature
carries with it some unique information. Here are two most correlated
features are that of Family size and Parch (Parents and Children). I'll
still leave both features in for the purposes of this exercise.

\paragraph{Pairplots}\label{pairplots}

Finally let us generate some pairplots to observe the distribution of
data from one feature to the other. Once again we use Seaborn to help
us.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{g} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{train}\PY{p}{[}\PY{p}{[}\PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Survived}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Pclass}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Parch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fare}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Embarked}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FamilySize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{u}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Survived}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{palette} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seismic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{size}\PY{o}{=}\PY{l+m+mf}{1.2}\PY{p}{,}\PY{n}{diag\PYZus{}kind} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{diag\PYZus{}kws}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{shade}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}\PY{n}{plot\PYZus{}kws}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{s}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{p}{)}
        \PY{n}{g}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xticklabels}\PY{o}{=}\PY{p}{[}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} <seaborn.axisgrid.PairGrid at 0x103332fd0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Ensembling \& Stacking
models}\label{ensembling-stacking-models}

Finally after that brief whirlwind detour with regards to feature
engineering and formatting, we finally arrive at the meat and gist of
the this notebook.

Creating a Stacking ensemble!

\paragraph{Helpers via Python Classes}\label{helpers-via-python-classes}

Here we invoke the use of Python's classes to help make it more
convenient for us. For any newcomers to programming, one normally hears
Classes being used in conjunction with Object-Oriented Programming
(OOP). In short, a class helps to extend some code/program for creating
objects (variables for old-school peeps) as well as to implement
functions and methods specific to that class.

In the section of code below, we essentially write a class SklearnHelper
that allows one to extend the inbuilt methods (such as train, predict
and fit) common to all the Sklearn classifiers. Therefore this cuts out
redundancy as won't need to write the same methods five times if we
wanted to invoke five different classifiers.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Some useful parameters which will come in handy later on}
        \PY{n}{ntrain} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{ntest} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{SEED} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} for reproducibility}
        \PY{n}{NFOLDS} \PY{o}{=} \PY{l+m+mi}{5} \PY{c+c1}{\PYZsh{} set folds for out\PYZhy{}of\PYZhy{}fold prediction}
        \PY{n}{kf} \PY{o}{=} \PY{n}{KFold}\PY{p}{(}\PY{n}{ntrain}\PY{p}{,} \PY{n}{n\PYZus{}folds}\PY{o}{=} \PY{n}{NFOLDS}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Class to extend the Sklearn classifier}
        \PY{k}{class} \PY{n+nc}{SklearnHelper}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{clf}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{params}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
                \PY{n}{params}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{seed}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{clf} \PY{o}{=} \PY{n}{clf}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{params}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{:}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{feature\PYZus{}importances}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} Class to extend XGboost classifer}
\end{Verbatim}


    Bear with me for those who already know this but for people who have not
created classes or objects in Python before, let me explain what the
code given above does. In creating my base classifiers, I will only use
the models already present in the Sklearn library and therefore only
extend the class for that.

def init : Python standard for invoking the default constructor for the
class. This means that when you want to create an object (classifier),
you have to give it the parameters of clf (what sklearn classifier you
want), seed (random seed) and params (parameters for the classifiers).

The rest of the code are simply methods of the class which simply call
the corresponding methods already existing within the sklearn
classifiers. Essentially, we have created a wrapper class to extend the
various Sklearn classifiers so that this should help us reduce having to
write the same code over and over when we implement multiple learners to
our stacker.

\paragraph{Out-of-Fold Predictions}\label{out-of-fold-predictions}

Now as alluded to above in the introductory section, stacking uses
predictions of base classifiers as input for training to a second-level
model. However one cannot simply train the base models on the full
training data, generate predictions on the full test set and then output
these for the second-level training. This runs the risk of your base
model predictions already having "seen" the test set and therefore
overfitting when feeding these predictions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}oof}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{)}\PY{p}{:}
             \PY{n}{oof\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{ntrain}\PY{p}{,}\PY{p}{)}\PY{p}{)}
             \PY{n}{oof\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{ntest}\PY{p}{,}\PY{p}{)}\PY{p}{)}
             \PY{n}{oof\PYZus{}test\PYZus{}skf} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{p}{(}\PY{n}{NFOLDS}\PY{p}{,} \PY{n}{ntest}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{train\PYZus{}index}\PY{p}{,} \PY{n}{test\PYZus{}index}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{kf}\PY{p}{)}\PY{p}{:}
                 \PY{n}{x\PYZus{}tr} \PY{o}{=} \PY{n}{x\PYZus{}train}\PY{p}{[}\PY{n}{train\PYZus{}index}\PY{p}{]}
                 \PY{n}{y\PYZus{}tr} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{train\PYZus{}index}\PY{p}{]}
                 \PY{n}{x\PYZus{}te} \PY{o}{=} \PY{n}{x\PYZus{}train}\PY{p}{[}\PY{n}{test\PYZus{}index}\PY{p}{]}
         
                 \PY{n}{clf}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{x\PYZus{}tr}\PY{p}{,} \PY{n}{y\PYZus{}tr}\PY{p}{)}
         
                 \PY{n}{oof\PYZus{}train}\PY{p}{[}\PY{n}{test\PYZus{}index}\PY{p}{]} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}te}\PY{p}{)}
                 \PY{n}{oof\PYZus{}test\PYZus{}skf}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}
         
             \PY{n}{oof\PYZus{}test}\PY{p}{[}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{oof\PYZus{}test\PYZus{}skf}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             \PY{k}{return} \PY{n}{oof\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{oof\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Generating our Base First-Level
Models}\label{generating-our-base-first-level-models}

So now let us prepare five learning models as our first level
classification. These models can all be conveniently invoked via the
Sklearn library and are listed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random Forest classifier\\
\item
  Extra Trees classifier\\
\item
  AdaBoost classifer
\item
  Gradient Boosting classifer
\item
  Support Vector Machine \#\#\#\# Parameters
\end{enumerate}

Just a quick summary of the parameters that we will be listing here for
completeness,

\textbf{n\_jobs} : Number of cores used for the training process. If set
to -1, all cores are used.

\textbf{n\_estimators} : Number of classification trees in your learning
model ( set to 10 per default)

\textbf{max\_depth} : Maximum depth of tree, or how much a node should
be expanded. Beware if set to too high a number would run the risk of
overfitting as one would be growing the tree too deep

\textbf{verbose} : Controls whether you want to output any text during
the learning process. A value of 0 suppresses all text while a value of
3 outputs the tree learning process at every iteration.

Please check out the full description via the official Sklearn website.
There you will find that there are a whole host of other useful
parameters that you can play around with.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} Put in our parameters for said classifiers}
         \PY{c+c1}{\PYZsh{} Random Forest parameters}
         \PY{n}{rf\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}jobs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{500}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warm\PYZus{}start}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,} 
              \PY{c+c1}{\PYZsh{}\PYZsq{}max\PYZus{}features\PYZsq{}: 0.2,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{6}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sqrt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verbose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}
         \PY{p}{\PYZcb{}}
         
         \PY{c+c1}{\PYZsh{} Extra Trees Parameters}
         \PY{n}{et\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}jobs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{500}\PY{p}{,}
             \PY{c+c1}{\PYZsh{}\PYZsq{}max\PYZus{}features\PYZsq{}: 0.5,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{8}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verbose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}
         \PY{p}{\PYZcb{}}
         
         \PY{c+c1}{\PYZsh{} AdaBoost parameters}
         \PY{n}{ada\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{500}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mf}{0.75}
         \PY{p}{\PYZcb{}}
         
         \PY{c+c1}{\PYZsh{} Gradient Boosting parameters}
         \PY{n}{gb\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{500}\PY{p}{,}
              \PY{c+c1}{\PYZsh{}\PYZsq{}max\PYZus{}features\PYZsq{}: 0.2,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{5}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verbose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}
         \PY{p}{\PYZcb{}}
         
         \PY{c+c1}{\PYZsh{} Support Vector Classifier parameters }
         \PY{n}{svc\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{l+m+mf}{0.025}
             \PY{p}{\PYZcb{}}
\end{Verbatim}


    Furthermore, since having mentioned about Objects and classes within the
OOP framework, let us now create 5 objects that represent our 5 learning
models via our Helper Sklearn Class we defined earlier.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Create 5 objects that represent our 4 models}
         \PY{n}{rf} \PY{o}{=} \PY{n}{SklearnHelper}\PY{p}{(}\PY{n}{clf}\PY{o}{=}\PY{n}{RandomForestClassifier}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{n}{SEED}\PY{p}{,} \PY{n}{params}\PY{o}{=}\PY{n}{rf\PYZus{}params}\PY{p}{)}
         \PY{n}{et} \PY{o}{=} \PY{n}{SklearnHelper}\PY{p}{(}\PY{n}{clf}\PY{o}{=}\PY{n}{ExtraTreesClassifier}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{n}{SEED}\PY{p}{,} \PY{n}{params}\PY{o}{=}\PY{n}{et\PYZus{}params}\PY{p}{)}
         \PY{n}{ada} \PY{o}{=} \PY{n}{SklearnHelper}\PY{p}{(}\PY{n}{clf}\PY{o}{=}\PY{n}{AdaBoostClassifier}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{n}{SEED}\PY{p}{,} \PY{n}{params}\PY{o}{=}\PY{n}{ada\PYZus{}params}\PY{p}{)}
         \PY{n}{gb} \PY{o}{=} \PY{n}{SklearnHelper}\PY{p}{(}\PY{n}{clf}\PY{o}{=}\PY{n}{GradientBoostingClassifier}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{n}{SEED}\PY{p}{,} \PY{n}{params}\PY{o}{=}\PY{n}{gb\PYZus{}params}\PY{p}{)}
         \PY{n}{svc} \PY{o}{=} \PY{n}{SklearnHelper}\PY{p}{(}\PY{n}{clf}\PY{o}{=}\PY{n}{SVC}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{n}{SEED}\PY{p}{,} \PY{n}{params}\PY{o}{=}\PY{n}{svc\PYZus{}params}\PY{p}{)}
\end{Verbatim}


    \paragraph{Creating NumPy arrays out of our train and test
sets}\label{creating-numpy-arrays-out-of-our-train-and-test-sets}

Great. Having prepared our first layer base models as such, we can now
ready the training and test test data for input into our classifiers by
generating NumPy arrays out of their original dataframes as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models}
         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Survived}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
         \PY{n}{train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Survived}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{values} \PY{c+c1}{\PYZsh{} Creates an array of the train data}
         \PY{n}{x\PYZus{}test} \PY{o}{=} \PY{n}{test}\PY{o}{.}\PY{n}{values} \PY{c+c1}{\PYZsh{} Creats an array of the test data}
\end{Verbatim}


    \paragraph{Output of the First level
Predictions}\label{output-of-the-first-level-predictions}

We now feed the training and test data into our 5 base classifiers and
use the Out-of-Fold prediction function we defined earlier to generate
our first level predictions. Allow a handful of minutes for the chunk of
code below to run.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Create our OOF train and test predictions. These base results will be used as new features}
         \PY{n}{et\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{et\PYZus{}oof\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}oof}\PY{p}{(}\PY{n}{et}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{)} \PY{c+c1}{\PYZsh{} Extra Trees}
         \PY{n}{rf\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{rf\PYZus{}oof\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}oof}\PY{p}{(}\PY{n}{rf}\PY{p}{,}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{)} \PY{c+c1}{\PYZsh{} Random Forest}
         \PY{n}{ada\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{ada\PYZus{}oof\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}oof}\PY{p}{(}\PY{n}{ada}\PY{p}{,} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{)} \PY{c+c1}{\PYZsh{} AdaBoost }
         \PY{n}{gb\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{gb\PYZus{}oof\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}oof}\PY{p}{(}\PY{n}{gb}\PY{p}{,}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{)} \PY{c+c1}{\PYZsh{} Gradient Boost}
         \PY{n}{svc\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{svc\PYZus{}oof\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}oof}\PY{p}{(}\PY{n}{svc}\PY{p}{,}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{)} \PY{c+c1}{\PYZsh{} Support Vector Classifier}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training is complete}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Training is complete

    \end{Verbatim}

    \paragraph{Feature importances generated from the different
classifiers}\label{feature-importances-generated-from-the-different-classifiers}

Now having learned our the first-level classifiers, we can utilise a
very nifty feature of the Sklearn models and that is to output the
importances of the various features in the training and test sets with
one very simple line of code.

As per the Sklearn documentation, most of the classifiers are built in
with an attribute which returns feature importances by simply typing in
.featureimportances. Therefore we will invoke this very useful attribute
via our function earliand plot the feature importances as such

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{rf\PYZus{}feature} \PY{o}{=} \PY{n}{rf}\PY{o}{.}\PY{n}{feature\PYZus{}importances}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{et\PYZus{}feature} \PY{o}{=} \PY{n}{et}\PY{o}{.}\PY{n}{feature\PYZus{}importances}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{ada\PYZus{}feature} \PY{o}{=} \PY{n}{ada}\PY{o}{.}\PY{n}{feature\PYZus{}importances}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{gb\PYZus{}feature} \PY{o}{=} \PY{n}{gb}\PY{o}{.}\PY{n}{feature\PYZus{}importances}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0.12463504 0.19960499 0.03321957 0.02046529 0.07208647 0.02345534
 0.10923858 0.06477004 0.06647458 0.01377533 0.27227477]
[0.11802927 0.37542257 0.03111556 0.01746033 0.05619495 0.02812711
 0.04672701 0.08306522 0.04600967 0.02171591 0.17613239]
[0.034 0.012 0.016 0.06  0.042 0.01  0.692 0.014 0.048 0.004 0.068]
[0.07740141 0.02682134 0.09959914 0.02881164 0.10415811 0.04805534
 0.41566283 0.01775062 0.07466152 0.02022939 0.08684865]

    \end{Verbatim}

    So I have not yet figured out how to assign and store the feature
importances outright. Therefore I'll print out the values from the code
above and then simply copy and paste into Python lists as below (sorry
for the lousy hack)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{rf\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.10474135}\PY{p}{,}  \PY{l+m+mf}{0.21837029}\PY{p}{,}  \PY{l+m+mf}{0.04432652}\PY{p}{,}  \PY{l+m+mf}{0.02249159}\PY{p}{,}  \PY{l+m+mf}{0.05432591}\PY{p}{,}  \PY{l+m+mf}{0.02854371}
           \PY{p}{,}\PY{l+m+mf}{0.07570305}\PY{p}{,}  \PY{l+m+mf}{0.01088129} \PY{p}{,} \PY{l+m+mf}{0.24247496}\PY{p}{,}  \PY{l+m+mf}{0.13685733} \PY{p}{,} \PY{l+m+mf}{0.06128402}\PY{p}{]}
         \PY{n}{et\PYZus{}features} \PY{o}{=} \PY{p}{[} \PY{l+m+mf}{0.12165657}\PY{p}{,}  \PY{l+m+mf}{0.37098307}  \PY{p}{,}\PY{l+m+mf}{0.03129623} \PY{p}{,} \PY{l+m+mf}{0.01591611} \PY{p}{,} \PY{l+m+mf}{0.05525811} \PY{p}{,} \PY{l+m+mf}{0.028157}
           \PY{p}{,}\PY{l+m+mf}{0.04589793} \PY{p}{,} \PY{l+m+mf}{0.02030357} \PY{p}{,} \PY{l+m+mf}{0.17289562} \PY{p}{,} \PY{l+m+mf}{0.04853517}\PY{p}{,}  \PY{l+m+mf}{0.08910063}\PY{p}{]}
         \PY{n}{ada\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.028} \PY{p}{,}   \PY{l+m+mf}{0.008}  \PY{p}{,}      \PY{l+m+mf}{0.012}   \PY{p}{,}     \PY{l+m+mf}{0.05866667}\PY{p}{,}   \PY{l+m+mf}{0.032} \PY{p}{,}       \PY{l+m+mf}{0.008}
           \PY{p}{,}\PY{l+m+mf}{0.04666667} \PY{p}{,}  \PY{l+m+mf}{0.}     \PY{p}{,}      \PY{l+m+mf}{0.05733333}\PY{p}{,}   \PY{l+m+mf}{0.73866667}\PY{p}{,}   \PY{l+m+mf}{0.01066667}\PY{p}{]}
         \PY{n}{gb\PYZus{}features} \PY{o}{=} \PY{p}{[} \PY{l+m+mf}{0.06796144} \PY{p}{,} \PY{l+m+mf}{0.03889349} \PY{p}{,} \PY{l+m+mf}{0.07237845} \PY{p}{,} \PY{l+m+mf}{0.02628645} \PY{p}{,} \PY{l+m+mf}{0.11194395}\PY{p}{,}  \PY{l+m+mf}{0.04778854}
           \PY{p}{,}\PY{l+m+mf}{0.05965792} \PY{p}{,} \PY{l+m+mf}{0.02774745}\PY{p}{,}  \PY{l+m+mf}{0.07462718}\PY{p}{,}  \PY{l+m+mf}{0.4593142} \PY{p}{,}  \PY{l+m+mf}{0.01340093}\PY{p}{]}
\end{Verbatim}


    Create a dataframe from the lists containing the feature importance data
for easy plotting via the Plotly package.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} array(['Pclass', 'Sex', 'Age', 'Parch', 'Fare', 'Embarked', 'Name\_length',
                'Has\_Cabin', 'FamilySize', 'IsAlone', 'Title'], dtype=object)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{cols} \PY{o}{=} \PY{n}{train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
         \PY{c+c1}{\PYZsh{} Create a dataframe with features}
         \PY{n}{feature\PYZus{}dataframe} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{cols}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random Forest feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{rf\PYZus{}features}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Extra Trees  feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{et\PYZus{}features}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AdaBoost feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{ada\PYZus{}features}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Boost feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{gb\PYZus{}features}
             \PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \paragraph{Interactive feature importances via Plotly
scatterplots}\label{interactive-feature-importances-via-plotly-scatterplots}

I'll use the interactive Plotly package at this juncture to visualise
the feature importances values of the different classifiers via a plotly
scatter plot by calling "Scatter" as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Scatter plot }
         \PY{n}{trace} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Scatter}\PY{p}{(}
             \PY{n}{y} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random Forest feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{x} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{markers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{marker}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{sizemode} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diameter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{sizeref} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
                 \PY{n}{size} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}       size= feature\PYZus{}dataframe[\PYZsq{}AdaBoost feature importances\PYZsq{}].values,}
                 \PY{c+c1}{\PYZsh{}color = np.random.randn(500), \PYZsh{}set color equal to a variable}
                 \PY{n}{color} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random Forest feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                 \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Portland}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{showscale}\PY{o}{=}\PY{k+kc}{True}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{text} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{p}{)}
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{trace}\PY{p}{]}
         
         \PY{n}{layout}\PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Layout}\PY{p}{(}
             \PY{n}{autosize}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,}
             \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random Forest Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{hovermode}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{closest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}     xaxis= dict(}
         \PY{c+c1}{\PYZsh{}         title= \PYZsq{}Pop\PYZsq{},}
         \PY{c+c1}{\PYZsh{}         ticklen= 5,}
         \PY{c+c1}{\PYZsh{}         zeroline= False,}
         \PY{c+c1}{\PYZsh{}         gridwidth= 2,}
         \PY{c+c1}{\PYZsh{}     ),}
             \PY{n}{yaxis}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{ticklen}\PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}
                 \PY{n}{gridwidth}\PY{o}{=} \PY{l+m+mi}{2}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{showlegend}\PY{o}{=} \PY{k+kc}{False}
         \PY{p}{)}
         \PY{n}{fig} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{n}{layout}\PY{p}{)}
         \PY{n}{py}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{fig}\PY{p}{,}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scatter2010}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Scatter plot }
         \PY{n}{trace} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Scatter}\PY{p}{(}
             \PY{n}{y} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Extra Trees  feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{x} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{markers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{marker}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{sizemode} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diameter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{sizeref} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
                 \PY{n}{size} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}       size= feature\PYZus{}dataframe[\PYZsq{}AdaBoost feature importances\PYZsq{}].values,}
                 \PY{c+c1}{\PYZsh{}color = np.random.randn(500), \PYZsh{}set color equal to a variable}
                 \PY{n}{color} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Extra Trees  feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                 \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Portland}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{showscale}\PY{o}{=}\PY{k+kc}{True}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{text} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{p}{)}
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{trace}\PY{p}{]}
         
         \PY{n}{layout}\PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Layout}\PY{p}{(}
             \PY{n}{autosize}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,}
             \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Extra Trees Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{hovermode}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{closest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}     xaxis= dict(}
         \PY{c+c1}{\PYZsh{}         title= \PYZsq{}Pop\PYZsq{},}
         \PY{c+c1}{\PYZsh{}         ticklen= 5,}
         \PY{c+c1}{\PYZsh{}         zeroline= False,}
         \PY{c+c1}{\PYZsh{}         gridwidth= 2,}
         \PY{c+c1}{\PYZsh{}     ),}
             \PY{n}{yaxis}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{ticklen}\PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}
                 \PY{n}{gridwidth}\PY{o}{=} \PY{l+m+mi}{2}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{showlegend}\PY{o}{=} \PY{k+kc}{False}
         \PY{p}{)}
         \PY{n}{fig} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{n}{layout}\PY{p}{)}
         \PY{n}{py}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{fig}\PY{p}{,}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scatter2010}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Scatter plot }
         \PY{n}{trace} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Scatter}\PY{p}{(}
             \PY{n}{y} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AdaBoost feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{x} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{markers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{marker}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{sizemode} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diameter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{sizeref} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
                 \PY{n}{size} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}       size= feature\PYZus{}dataframe[\PYZsq{}AdaBoost feature importances\PYZsq{}].values,}
                 \PY{c+c1}{\PYZsh{}color = np.random.randn(500), \PYZsh{}set color equal to a variable}
                 \PY{n}{color} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AdaBoost feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                 \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Portland}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{showscale}\PY{o}{=}\PY{k+kc}{True}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{text} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{p}{)}
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{trace}\PY{p}{]}
         
         \PY{n}{layout}\PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Layout}\PY{p}{(}
             \PY{n}{autosize}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,}
             \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AdaBoost Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{hovermode}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{closest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}     xaxis= dict(}
         \PY{c+c1}{\PYZsh{}         title= \PYZsq{}Pop\PYZsq{},}
         \PY{c+c1}{\PYZsh{}         ticklen= 5,}
         \PY{c+c1}{\PYZsh{}         zeroline= False,}
         \PY{c+c1}{\PYZsh{}         gridwidth= 2,}
         \PY{c+c1}{\PYZsh{}     ),}
             \PY{n}{yaxis}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{ticklen}\PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}
                 \PY{n}{gridwidth}\PY{o}{=} \PY{l+m+mi}{2}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{showlegend}\PY{o}{=} \PY{k+kc}{False}
         \PY{p}{)}
         \PY{n}{fig} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{n}{layout}\PY{p}{)}
         \PY{n}{py}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{fig}\PY{p}{,}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scatter2010}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Scatter plot }
         \PY{n}{trace} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Scatter}\PY{p}{(}
             \PY{n}{y} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Boost feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{x} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
             \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{markers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{marker}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{sizemode} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diameter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{sizeref} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
                 \PY{n}{size} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}       size= feature\PYZus{}dataframe[\PYZsq{}AdaBoost feature importances\PYZsq{}].values,}
                 \PY{c+c1}{\PYZsh{}color = np.random.randn(500), \PYZsh{}set color equal to a variable}
                 \PY{n}{color} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Boost feature importances}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                 \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Portland}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{showscale}\PY{o}{=}\PY{k+kc}{True}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{text} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{p}{)}
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{trace}\PY{p}{]}
         
         \PY{n}{layout}\PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Layout}\PY{p}{(}
             \PY{n}{autosize}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,}
             \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Boosting Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{hovermode}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{closest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}     xaxis= dict(}
         \PY{c+c1}{\PYZsh{}         title= \PYZsq{}Pop\PYZsq{},}
         \PY{c+c1}{\PYZsh{}         ticklen= 5,}
         \PY{c+c1}{\PYZsh{}         zeroline= False,}
         \PY{c+c1}{\PYZsh{}         gridwidth= 2,}
         \PY{c+c1}{\PYZsh{}     ),}
             \PY{n}{yaxis}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{ticklen}\PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}
                 \PY{n}{gridwidth}\PY{o}{=} \PY{l+m+mi}{2}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{showlegend}\PY{o}{=} \PY{k+kc}{False}
         \PY{p}{)}
         \PY{n}{fig} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{n}{layout}\PY{p}{)}
         \PY{n}{py}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{fig}\PY{p}{,}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scatter2010}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    
    
    
    
    
    
    
    
    Now let us calculate the mean of all the feature importances and store
it as a new column in the feature importance dataframe.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} Create the new column containing the average of values}
         
         \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} axis = 1 computes the mean row\PYZhy{}wise}
         \PY{n}{feature\PYZus{}dataframe}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}20}]:}   features  Random Forest feature importances  \textbackslash{}
         0   Pclass                           0.104741   
         1      Sex                           0.218370   
         2      Age                           0.044327   
         
            Extra Trees  feature importances  AdaBoost feature importances  \textbackslash{}
         0                          0.121657                         0.028   
         1                          0.370983                         0.008   
         2                          0.031296                         0.012   
         
            Gradient Boost feature importances      mean  
         0                            0.067961  0.080590  
         1                            0.038893  0.159062  
         2                            0.072378  0.040000  
\end{Verbatim}
            
    \paragraph{Plotly Barplot of Average Feature
Importances}\label{plotly-barplot-of-average-feature-importances}

Having obtained the mean feature importance across all our classifiers,
we can plot them into a Plotly bar plot as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{y} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{x} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{go}\PY{o}{.}\PY{n}{Bar}\PY{p}{(}
                     \PY{n}{x}\PY{o}{=} \PY{n}{x}\PY{p}{,}
                      \PY{n}{y}\PY{o}{=} \PY{n}{y}\PY{p}{,}
                     \PY{n}{width} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{,}
                     \PY{n}{marker}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                        \PY{n}{color} \PY{o}{=} \PY{n}{feature\PYZus{}dataframe}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                     \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Portland}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{showscale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                     \PY{n}{reversescale} \PY{o}{=} \PY{k+kc}{False}
                     \PY{p}{)}\PY{p}{,}
                     \PY{n}{opacity}\PY{o}{=}\PY{l+m+mf}{0.6}
                 \PY{p}{)}\PY{p}{]}
         
         \PY{n}{layout}\PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Layout}\PY{p}{(}
             \PY{n}{autosize}\PY{o}{=} \PY{k+kc}{True}\PY{p}{,}
             \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Barplots of Mean Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{n}{hovermode}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{closest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         \PY{c+c1}{\PYZsh{}     xaxis= dict(}
         \PY{c+c1}{\PYZsh{}         title= \PYZsq{}Pop\PYZsq{},}
         \PY{c+c1}{\PYZsh{}         ticklen= 5,}
         \PY{c+c1}{\PYZsh{}         zeroline= False,}
         \PY{c+c1}{\PYZsh{}         gridwidth= 2,}
         \PY{c+c1}{\PYZsh{}     ),}
             \PY{n}{yaxis}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}
                 \PY{n}{title}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{ticklen}\PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}
                 \PY{n}{gridwidth}\PY{o}{=} \PY{l+m+mi}{2}
             \PY{p}{)}\PY{p}{,}
             \PY{n}{showlegend}\PY{o}{=} \PY{k+kc}{False}
         \PY{p}{)}
         \PY{n}{fig} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{data}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{n}{layout}\PY{p}{)}
         \PY{n}{py}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{fig}\PY{p}{,} \PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar\PYZhy{}direct\PYZhy{}labels}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    
    
    \subsubsection{Second-Level Predictions from the First-level
Output}\label{second-level-predictions-from-the-first-level-output}

\paragraph{First-level output as new
features}\label{first-level-output-as-new-features}

Having now obtained our first-level predictions, one can think of it as
essentially building a new set of features to be used as training data
for the next classifier. As per the code below, we are therefore having
as our new columns the first-level predictions from our earlier
classifiers and we train the next classifier on this.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{base\PYZus{}predictions\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RandomForest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{rf\PYZus{}oof\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ExtraTrees}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{et\PYZus{}oof\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AdaBoost}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{ada\PYZus{}oof\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,}
               \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GradientBoost}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{gb\PYZus{}oof\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}
             \PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{base\PYZus{}predictions\PYZus{}train}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:}    RandomForest  ExtraTrees  AdaBoost  GradientBoost
         0           0.0         0.0       0.0            0.0
         1           1.0         1.0       1.0            1.0
         2           0.0         0.0       1.0            1.0
         3           1.0         1.0       1.0            1.0
         4           0.0         0.0       0.0            0.0
\end{Verbatim}
            
    \paragraph{Correlation Heatmap of the Second Level Training
set}\label{correlation-heatmap-of-the-second-level-training-set}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{data} \PY{o}{=} \PY{p}{[}
             \PY{n}{go}\PY{o}{.}\PY{n}{Heatmap}\PY{p}{(}
                 \PY{n}{z}\PY{o}{=} \PY{n}{base\PYZus{}predictions\PYZus{}train}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values} \PY{p}{,}
                 \PY{n}{x}\PY{o}{=}\PY{n}{base\PYZus{}predictions\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                 \PY{n}{y}\PY{o}{=} \PY{n}{base\PYZus{}predictions\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                   \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{n}{showscale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                     \PY{n}{reversescale} \PY{o}{=} \PY{k+kc}{True}
             \PY{p}{)}
         \PY{p}{]}
         \PY{n}{py}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labelled\PYZhy{}heatmap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    
    
    There have been quite a few articles and Kaggle competition winner
stories about the merits of having trained models that are more
uncorrelated with one another producing better scores.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{x\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(} \PY{n}{et\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{rf\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{ada\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{gb\PYZus{}oof\PYZus{}train}\PY{p}{,} \PY{n}{svc\PYZus{}oof\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{x\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(} \PY{n}{et\PYZus{}oof\PYZus{}test}\PY{p}{,} \PY{n}{rf\PYZus{}oof\PYZus{}test}\PY{p}{,} \PY{n}{ada\PYZus{}oof\PYZus{}test}\PY{p}{,} \PY{n}{gb\PYZus{}oof\PYZus{}test}\PY{p}{,} \PY{n}{svc\PYZus{}oof\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{x\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}26}]:} array([0., 0., 0., 0., 0.])
\end{Verbatim}
            
    Having now concatenated and joined both the first-level train and test
predictions as x\_train and x\_test, we can now fit a second-level
learning model.

\paragraph{Second level learning model via
XGBoost}\label{second-level-learning-model-via-xgboost}

Here we choose the eXtremely famous library for boosted tree learning
model, XGBoost. It was built to optimize large-scale boosted tree
algorithms. For further information about the algorithm, check out the
official documentation.

Anyways, we call an XGBClassifier and fit it to the first-level train
and target data and use the learned model to predict the test data as
follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{gbm} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{XGBClassifier}\PY{p}{(}
             \PY{c+c1}{\PYZsh{}learning\PYZus{}rate = 0.02,}
          \PY{n}{n\PYZus{}estimators}\PY{o}{=} \PY{l+m+mi}{2000}\PY{p}{,}
          \PY{n}{max\PYZus{}depth}\PY{o}{=} \PY{l+m+mi}{4}\PY{p}{,}
          \PY{n}{min\PYZus{}child\PYZus{}weight}\PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,}
          \PY{c+c1}{\PYZsh{}gamma=1,}
          \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,}                        
          \PY{n}{subsample}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,}
          \PY{n}{colsample\PYZus{}bytree}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,}
          \PY{n}{objective}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary:logistic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
          \PY{n}{nthread}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
          \PY{n}{scale\PYZus{}pos\PYZus{}weight}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{predictions} \PY{o}{=} \PY{n}{gbm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{predictions}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} array([0, 1, 0, 0, 1])
\end{Verbatim}
            
    Just a quick run down of the XGBoost parameters used in the model:

\textbf{max\_depth} : How deep you want to grow your tree. Beware if set
to too high a number might run the risk of overfitting.

\textbf{gamma} : minimum loss reduction required to make a further
partition on a leaf node of the tree. The larger, the more conservative
the algorithm will be.

\textbf{eta} : step size shrinkage used in each boosting step to prevent
overfitting

\paragraph{Producing the Submission
file}\label{producing-the-submission-file}

Finally having trained and fit all our first-level and second-level
models, we can now output the predictions into the proper format for
submission to the Titanic competition as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Generate Submission File }
         \PY{n}{StackingSubmission} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PassengerId}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{PassengerId}\PY{p}{,}
                                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Survived}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{predictions} \PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{StackingSubmission}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{StackingSubmission.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    \paragraph{Steps for Further
Improvement}\label{steps-for-further-improvement}

As a closing remark it must be noted that the steps taken above just
show a very simple way of producing an ensemble stacker. You hear of
ensembles created at the highest level of Kaggle competitions which
involves monstrous combinations of stacked classifiers as well as levels
of stacking which go to more than 2 levels.

Some additional steps that may be taken to improve one's score could be:

Implementing a good cross-validation strategy in training the models to
find optimal parameter values Introduce a greater variety of base models
for learning. The more uncorrelated the results, the better the final
score. \#\#\#\# Conclusion I have this notebook has been helpful
somewhat in introducing a working script for stacking learning models.
Again credit must be extended to Faron and Sina.

For other excellent material on stacking or ensembling in general, refer
to the de-facto Must read article on the website MLWave: Kaggle
Ensembling Guide.

Till next time, Peace Out


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
